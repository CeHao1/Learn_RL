{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "WARNING:tensorflow:From <ipython-input-1-44c50e443fda>:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Model Defined at Here.\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__(name='basic_dqn')\n",
    "        # you can try different kernel initializer\n",
    "        self.fc1 = kl.Dense(32, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.fc2 = kl.Dense(32, activation='relu', kernel_initializer='he_uniform')\n",
    "        self.logits = kl.Dense(num_actions, name='q_values')\n",
    "\n",
    "    # forward propagation\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "    # a* = argmax_a' Q(s, a')\n",
    "    def action_value(self, obs):\n",
    "        q_values = self.predict(obs)\n",
    "        best_action = np.argmax(q_values, axis=-1)\n",
    "        return best_action[0], q_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test whether the model works\n",
    "def test_model():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    print('num_actions: ', env.action_space.n)\n",
    "    model = Model(env.action_space.n)\n",
    "\n",
    "    obs = env.reset()\n",
    "    print('obs_shape: ', obs.shape)\n",
    "    print(obs)\n",
    "\n",
    "    # tensorflow 2.0: no feed_dict or tf.Session() needed at all\n",
    "    best_action, q_values = model.action_value(obs[None])\n",
    "    print('res of test model: ', best_action, q_values)  # 0 [ 0.00896799 -0.02111824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:  # Deep Q-Network\n",
    "    def __init__(self, model, target_model, env, buffer_size=100, learning_rate=.0015, epsilon=.1, epsilon_dacay=0.995,\n",
    "                 min_epsilon=.01, gamma=.95, batch_size=4, target_update_iter=400, train_nums=5000, start_learning=10):\n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        # print(id(self.model), id(self.target_model))  # to make sure the two models don't update simultaneously\n",
    "        # gradient clip\n",
    "        opt = ko.Adam(learning_rate=learning_rate, clipvalue=10.0)  # do gradient clip\n",
    "        self.model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "        # parameters\n",
    "        self.env = env                              # gym environment\n",
    "        self.lr = learning_rate                     # learning step\n",
    "        self.epsilon = epsilon                      # e-greedy when exploring\n",
    "        self.epsilon_decay = epsilon_dacay          # epsilon decay rate\n",
    "        self.min_epsilon = min_epsilon              # minimum epsilon\n",
    "        self.gamma = gamma                          # discount rate\n",
    "        self.batch_size = batch_size                # batch_size\n",
    "        self.target_update_iter = target_update_iter    # target network update period\n",
    "        self.train_nums = train_nums                # total training steps\n",
    "        self.num_in_buffer = 0                      # transition's num in buffer\n",
    "        self.buffer_size = buffer_size              # replay buffer size\n",
    "        self.start_learning = start_learning        # step to begin learning(no update before that step)\n",
    "\n",
    "        # replay buffer params [(s, a, r, ns, done), ...]\n",
    "        self.obs = np.empty((self.buffer_size,) + self.env.reset().shape)\n",
    "        self.actions = np.empty((self.buffer_size), dtype=np.int8)\n",
    "        self.rewards = np.empty((self.buffer_size), dtype=np.float32)\n",
    "        self.dones = np.empty((self.buffer_size), dtype=np.bool)\n",
    "        self.next_states = np.empty((self.buffer_size,) + self.env.reset().shape)\n",
    "        self.next_idx = 0\n",
    "\n",
    "    def train(self):\n",
    "        # initialize the initial observation of the agent\n",
    "        obs = self.env.reset()\n",
    "        for t in range(1, self.train_nums):\n",
    "            best_action, q_values = self.model.action_value(obs[None])  # input the obs to the network model\n",
    "            action = self.get_action(best_action)   # get the real action\n",
    "            next_obs, reward, done, info = self.env.step(action)    # take the action in the env to return s', r, done\n",
    "            self.store_transition(obs, action, reward, next_obs, done)  # store that transition into replay butter\n",
    "            self.num_in_buffer = min(self.num_in_buffer + 1, self.buffer_size)\n",
    "\n",
    "            if t > self.start_learning:  # start learning\n",
    "                losses = self.train_step()\n",
    "                if t % 1000 == 0:\n",
    "                    print('losses each 1000 steps: ', losses)\n",
    "\n",
    "            if t % self.target_update_iter == 0:\n",
    "                self.update_target_model()\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "            else:\n",
    "                obs = next_obs\n",
    "\n",
    "    def train_step(self):\n",
    "        idxes = self.sample(self.batch_size)\n",
    "        s_batch = self.obs[idxes]\n",
    "        a_batch = self.actions[idxes]\n",
    "        r_batch = self.rewards[idxes]\n",
    "        ns_batch = self.next_states[idxes]\n",
    "        done_batch = self.dones[idxes]\n",
    "\n",
    "        target_q = r_batch + self.gamma * np.amax(self.get_target_value(ns_batch), axis=1) * (1 - done_batch)\n",
    "        target_f = self.model.predict(s_batch)\n",
    "        for i, val in enumerate(a_batch):\n",
    "            target_f[i][val] = target_q[i]\n",
    "\n",
    "        losses = self.model.train_on_batch(s_batch, target_f)\n",
    " \n",
    "        return losses\n",
    "\n",
    "    def evalation(self, env, render=True):\n",
    "        obs, done, ep_reward = env.reset(), False, 0\n",
    "        # one episode until done\n",
    "        while not done:\n",
    "            action, q_values = self.model.action_value(obs[None])  # Using [None] to extend its dimension (4,) -> (1, 4)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "#             if render:  # visually show\n",
    "#                 env.render()\n",
    "#             time.sleep(0.05)\n",
    "        env.close()\n",
    "        return ep_reward\n",
    "\n",
    "    # store transitions into replay butter\n",
    "    def store_transition(self, obs, action, reward, next_state, done):\n",
    "        n_idx = self.next_idx % self.buffer_size\n",
    "        self.obs[n_idx] = obs\n",
    "        self.actions[n_idx] = action\n",
    "        self.rewards[n_idx] = reward\n",
    "        self.next_states[n_idx] = next_state\n",
    "        self.dones[n_idx] = done\n",
    "        self.next_idx = (self.next_idx + 1) % self.buffer_size\n",
    "\n",
    "    # sample n different indexes\n",
    "    def sample(self, n):\n",
    "        assert n < self.num_in_buffer\n",
    "        res = []\n",
    "        while True:\n",
    "            num = np.random.randint(0, self.num_in_buffer)\n",
    "            if num not in res:\n",
    "                res.append(num)\n",
    "            if len(res) == n:\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    # e-greedy\n",
    "    def get_action(self, best_action):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return best_action\n",
    "\n",
    "    # assign the current network parameters to target network\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_target_value(self, obs):\n",
    "        return self.target_model.predict(obs)\n",
    "\n",
    "    def e_decay(self):\n",
    "        self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_actions:  2\n",
      "obs_shape:  (4,)\n",
      "[-0.04197973  0.01895008 -0.01270197  0.01064009]\n",
      "res of test model:  0 [ 0.0322018  -0.02901105]\n",
      "Before Training: 9 out of 200\n",
      "losses each 1000 steps:  0.46475347876548767\n",
      "losses each 1000 steps:  0.03826173022389412\n",
      "losses each 1000 steps:  0.3217830955982208\n",
      "losses each 1000 steps:  0.06398145109415054\n",
      "After Training: 45 out of 200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_model()\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    num_actions = env.action_space.n\n",
    "    model = Model(num_actions)\n",
    "    target_model = Model(num_actions)\n",
    "    agent = DQNAgent(model, target_model,  env)\n",
    "#     test before\n",
    "    rewards_sum = agent.evalation(env)\n",
    "    print(\"Before Training: %d out of 200\" % rewards_sum) # 9 out of 200\n",
    "\n",
    "    agent.train()\n",
    "    # test after\n",
    "    rewards_sum = agent.evalation(env)\n",
    "    print(\"After Training: %d out of 200\" % rewards_sum) # 200 out of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try more times\n",
    "rewards_sum = agent.evalation(env)\n",
    "print(\"After Training: %d out of 200\" % rewards_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.masscart = 2.0\n",
    "rewards_sum = agent.evalation(env)\n",
    "print(\"After Training: %d out of 200\" % rewards_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
